{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('TensorFlow25': conda)"
  },
  "interpreter": {
   "hash": "fe8cdd8b0905bc5dc811ca6f6f67baa88326482ee2d1fcebca307d9df55426eb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# English-to-Spanish translation with a sequence-to-sequence Transformer\n",
    "\n",
    "**Description:** Implementing a sequence-to-sequene Transformer and training it on a machine translation task.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, we'll build a sequence-to-sequence Transformer model, which\n",
    "we'll train on an English-to-Spanish machine translation task.\n",
    "\n",
    "You'll learn how to:\n",
    "\n",
    "- Vectorize text using the Keras `TextVectorization` layer.\n",
    "- Implement a `TransformerEncoder` layer, a `TransformerDecoder` layer,\n",
    "and a `PositionalEmbedding` layer.\n",
    "- Prepare data for training a sequence-to-sequence model.\n",
    "- Use the trained model to generate translations of never-seen-before\n",
    "input sentences (sequence-to-sequence inference).\n",
    "\n",
    "The code featured here is adapted from the book\n",
    "[Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)\n",
    "(chapter 11: Deep learning for text).\n",
    "The present example is fairly barebones, so for detailed explanations of\n",
    "how each building block works, as well as the theory behind Transformers,\n",
    "I recommend reading the book."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n"
   ]
  },
  {
   "source": [
    "## Downloading the data\n",
    "\n",
    "We'll be working with an English-to-Spanish translation dataset\n",
    "provided by [Anki](https://www.manythings.org/anki/). Let's download it:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname=\"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
   ]
  },
  {
   "source": [
    "## Parsing the data\n",
    "\n",
    "Each line contains an English sentence and its corresponding Spanish sentence.\n",
    "The English sentence is the *source sequence* and Spanish one is the *target sequence*.\n",
    "We prepend the token `\"[start]\"` and we append the token `\"[end]\"` to the Spanish sentence."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file, encoding='utf-8') as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    spa = \"[start] \" + spa + \" [end]\"\n",
    "    text_pairs.append((eng, spa))"
   ]
  },
  {
   "source": [
    "Here's what our sentence pairs look like:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(\"It's been a long time since I've had a real vacation.\", '[start] Hace mucho que no tengo unas vacaciones de verdad. [end]')\n('Tom accepted our offer.', '[start] Tomás aceptó nuestra oferta. [end]')\n('I trust him completely.', '[start] Confío plenamente en él. [end]')\n(\"He couldn't sleep because of the noise outside his window.\", '[start] Él no podía dormir por el ruido afuera de su ventana. [end]')\n('I will be free tomorrow afternoon.', '[start] Yo estaré desocupado mañana en la tarde. [end]')\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "source": [
    "Now, let's split the sentence pairs into a training set, a validation set,\n",
    "and a test set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "118964 total pairs\n83276 training pairs\n17844 validation pairs\n17844 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "source": [
    "## Vectorizing the text data\n",
    "\n",
    "We'll use two instances of the `TextVectorization` layer to vectorize the text\n",
    "data (one for English and one for Spanish),\n",
    "that is to say, to turn the original strings into integer sequences\n",
    "where each integer represents the index of a word in a vocabulary.\n",
    "\n",
    "The English layer will use the default string standardization (strip punctuation characters)\n",
    "and splitting scheme (split on whitespace), while\n",
    "the Spanish layer will use a custom standardization, where we add the character\n",
    "`\"¿\"` to the set of punctuation characters to be stripped.\n",
    "\n",
    "Note: in a production-grade machine translation model, I would not recommend\n",
    "stripping the punctuation characters in either language. Instead, I would recommend turning\n",
    "each punctuation character into its own token,\n",
    "which you could achieve by providing a custom `split` function to the `TextVectorization` layer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",
    ")\n",
    "spa_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_spa_texts = [pair[1] for pair in train_pairs]\n",
    "eng_vectorization.adapt(train_eng_texts)\n",
    "spa_vectorization.adapt(train_spa_texts)"
   ]
  },
  {
   "source": [
    "Next, we'll format our datasets.\n",
    "\n",
    "At each training step, the model will seek to predict target words N+1 (and beyond)\n",
    "using the source sentence and the target words 0 to N.\n",
    "\n",
    "As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n",
    "\n",
    "- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n",
    "`encoder_inputs` is the vectorized source sentence and `encoder_inputs` is the target sentence \"so far\",\n",
    "that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n",
    "- `target` is the target sentence offset by one step:\n",
    "it provides the next words in the target sentence -- what the model will try to predict."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_dataset(eng, spa):\n",
    "    eng = eng_vectorization(eng)\n",
    "    spa = spa_vectorization(spa)\n",
    "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "source": [
    "Let's take a quick look at the sequence shapes\n",
    "(we have batches of 64 pairs, and all sequences are 20 steps long):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (64, 20)\ninputs[\"decoder_inputs\"].shape: (64, 20)\ntargets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "source": [
    "## Building the model\n",
    "\n",
    "Our sequence-to-sequence Transformer consists of a `TransformerEncoder`\n",
    "and a `TransformerDecoder` chained together. To make the model aware of word order,\n",
    "we also use a `PositionalEmbedding` layer.\n",
    "\n",
    "The source sequence will be pass to the `TransformerEncoder`,\n",
    "which will produce a new representation of it.\n",
    "This new representation will then be passed\n",
    "to the `TransformerDecoder`, together with the target sequence so far (target words 0 to N).\n",
    "The `TransformerDecoder` will then seek to predict the next words in the target sequence (N+1 and beyond).\n",
    "\n",
    "A key detail that makes this possible is causal masking\n",
    "(see method `get_causal_attention_mask()` on the `TransformerDecoder`).\n",
    "The `TransformerDecoder` sees the entire sequences at once, and thus we must make\n",
    "sure that it only uses information from target tokens 0 to N when predicting token N+1\n",
    "(otherwise, it could use information from the future, which would\n",
    "result in a model that cannot be used at inference time)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n"
   ]
  },
  {
   "source": [
    "Next, we assemble the end-to-end model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 10,
   "outputs": []
  },
  {
   "source": [
    "## Training our model\n",
    "\n",
    "We'll use accuracy as a quick way to monitor training progress on the validation data.\n",
    "Note that machine translation typically uses BLEU scores as well as other metrics, rather than accuracy.\n",
    "\n",
    "Here we only train for 1 epoch, but to get the model to actually converge\n",
    "you should train for at least 30 epochs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positional_embedding (Positiona (None, None, 256)    3845120     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "transformer_encoder (Transforme (None, None, 256)    3155456     positional_embedding[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Functional)            (None, None, 15000)  12959640    decoder_inputs[0][0]             \n",
      "                                                                 transformer_encoder[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 19,960,216\n",
      "Trainable params: 19,960,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "1302/1302 [==============================] - 85s 62ms/step - loss: 1.7090 - accuracy: 0.4057 - val_loss: 1.3859 - val_accuracy: 0.4854\n",
      "Epoch 2/30\n",
      "1302/1302 [==============================] - 82s 63ms/step - loss: 1.3841 - accuracy: 0.5158 - val_loss: 1.2091 - val_accuracy: 0.5519\n",
      "Epoch 3/30\n",
      "1302/1302 [==============================] - 82s 63ms/step - loss: 1.2253 - accuracy: 0.5679 - val_loss: 1.1714 - val_accuracy: 0.5525\n",
      "Epoch 4/30\n",
      "1302/1302 [==============================] - 83s 64ms/step - loss: 1.1255 - accuracy: 0.6036 - val_loss: 1.1061 - val_accuracy: 0.5873\n",
      "Epoch 5/30\n",
      "1302/1302 [==============================] - 83s 63ms/step - loss: 1.0712 - accuracy: 0.6277 - val_loss: 1.0425 - val_accuracy: 0.6239\n",
      "Epoch 6/30\n",
      "1302/1302 [==============================] - 82s 63ms/step - loss: 1.0358 - accuracy: 0.6455 - val_loss: 1.0276 - val_accuracy: 0.6312\n",
      "Epoch 7/30\n",
      "1302/1302 [==============================] - 82s 63ms/step - loss: 1.0102 - accuracy: 0.6590 - val_loss: 1.0221 - val_accuracy: 0.6366\n",
      "Epoch 8/30\n",
      "1302/1302 [==============================] - 83s 63ms/step - loss: 0.9895 - accuracy: 0.6708 - val_loss: 1.0048 - val_accuracy: 0.6482\n",
      "Epoch 9/30\n",
      "1302/1302 [==============================] - 83s 64ms/step - loss: 0.9714 - accuracy: 0.6808 - val_loss: 1.0064 - val_accuracy: 0.6517\n",
      "Epoch 10/30\n",
      "1302/1302 [==============================] - 82s 63ms/step - loss: 0.9556 - accuracy: 0.6891 - val_loss: 1.0104 - val_accuracy: 0.6477\n",
      "Epoch 11/30\n",
      "1302/1302 [==============================] - 83s 63ms/step - loss: 0.9401 - accuracy: 0.6963 - val_loss: 1.0037 - val_accuracy: 0.6563\n",
      "Epoch 12/30\n",
      "1302/1302 [==============================] - 83s 63ms/step - loss: 0.9269 - accuracy: 0.7025 - val_loss: 1.0094 - val_accuracy: 0.6564\n",
      "Epoch 13/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.9141 - accuracy: 0.7081 - val_loss: 1.0125 - val_accuracy: 0.6543\n",
      "Epoch 14/30\n",
      "1302/1302 [==============================] - 80s 61ms/step - loss: 0.9014 - accuracy: 0.7140 - val_loss: 1.0149 - val_accuracy: 0.6562\n",
      "Epoch 15/30\n",
      "1302/1302 [==============================] - 82s 63ms/step - loss: 0.8877 - accuracy: 0.7189 - val_loss: 1.0093 - val_accuracy: 0.6599\n",
      "Epoch 16/30\n",
      "1302/1302 [==============================] - 82s 63ms/step - loss: 0.8789 - accuracy: 0.7233 - val_loss: 1.0123 - val_accuracy: 0.6594\n",
      "Epoch 17/30\n",
      "1302/1302 [==============================] - 82s 63ms/step - loss: 0.8678 - accuracy: 0.7281 - val_loss: 1.0120 - val_accuracy: 0.6630\n",
      "Epoch 18/30\n",
      "1302/1302 [==============================] - 83s 63ms/step - loss: 0.8567 - accuracy: 0.7324 - val_loss: 1.0307 - val_accuracy: 0.6616\n",
      "Epoch 19/30\n",
      "1302/1302 [==============================] - 83s 64ms/step - loss: 0.8480 - accuracy: 0.7359 - val_loss: 1.0240 - val_accuracy: 0.6609\n",
      "Epoch 20/30\n",
      "1302/1302 [==============================] - 80s 61ms/step - loss: 0.8372 - accuracy: 0.7400 - val_loss: 1.0358 - val_accuracy: 0.6579\n",
      "Epoch 21/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.8293 - accuracy: 0.7435 - val_loss: 1.0406 - val_accuracy: 0.6595\n",
      "Epoch 22/30\n",
      "1302/1302 [==============================] - 82s 63ms/step - loss: 0.8189 - accuracy: 0.7470 - val_loss: 1.0444 - val_accuracy: 0.6618\n",
      "Epoch 23/30\n",
      "1302/1302 [==============================] - 82s 63ms/step - loss: 0.8121 - accuracy: 0.7494 - val_loss: 1.0440 - val_accuracy: 0.6602\n",
      "Epoch 24/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.8034 - accuracy: 0.7524 - val_loss: 1.0472 - val_accuracy: 0.6615\n",
      "Epoch 25/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.7951 - accuracy: 0.7554 - val_loss: 1.0511 - val_accuracy: 0.6599\n",
      "Epoch 26/30\n",
      "1302/1302 [==============================] - 85s 65ms/step - loss: 0.7879 - accuracy: 0.7586 - val_loss: 1.0537 - val_accuracy: 0.6626\n",
      "Epoch 27/30\n",
      "1302/1302 [==============================] - 83s 64ms/step - loss: 0.7794 - accuracy: 0.7615 - val_loss: 1.0727 - val_accuracy: 0.6570\n",
      "Epoch 28/30\n",
      "1302/1302 [==============================] - 83s 64ms/step - loss: 0.7740 - accuracy: 0.7638 - val_loss: 1.0919 - val_accuracy: 0.6495\n",
      "Epoch 29/30\n",
      "1302/1302 [==============================] - 83s 64ms/step - loss: 0.7674 - accuracy: 0.7659 - val_loss: 1.0793 - val_accuracy: 0.6563\n",
      "Epoch 30/30\n",
      "1302/1302 [==============================] - 83s 64ms/step - loss: 0.7604 - accuracy: 0.7682 - val_loss: 1.0845 - val_accuracy: 0.6617\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fa2f6765b0>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "epochs = 30  # This should be at least 30 for convergence\n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  },
  {
   "source": [
    "## Decoding test sentences\n",
    "\n",
    "Finally, let's demonstrate how to translate brand new English sentences.\n",
    "We simply feed into the model the vectorized English sentence\n",
    "as well as the target token `\"[start]\"`, then we repeatedly generated the next token, until\n",
    "we hit the token `\"[end]\"`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I'm a great baseball fan.[start] soy un gran béisbol de béisbol [end]\n",
      "You may go anywhere.[start] puedes ir a ninguna parte [end]\n",
      "I like to study English.[start] me gusta estudiar inglés [end]\n",
      "I was at school.[start] yo estaba en la escuela [end]\n",
      "Tom doesn't have a wife.[start] tom no tiene mujer [end]\n",
      "The fog has lifted.[start] la [UNK] fue de correo [end]\n",
      "Tom lives a block away from us.[start] tom vive a una [UNK] de nosotros [end]\n",
      "He became very dangerous.[start] Él se hizo muy peligroso [end]\n",
      "Do you have any brothers or sisters?[start] tienes hermanos o hermanos [end]\n",
      "I never thought it'd be this hard to find an appropriate birthday gift for Tom.[start] nunca pensé que sería así de difícil encontrar un regalo de cumpleaños [end]\n",
      "Do you love Tom?[start] le encanta a tom [end]\n",
      "Today is the hottest day this year.[start] hoy es el día de la día de tokio [end]\n",
      "They are in the teachers' room.[start] ellos están en la habitación del suelo [end]\n",
      "We want to talk to Tom.[start] queremos hablar con tom [end]\n",
      "Please stop talking.[start] por favor deja de hablar [end]\n",
      "He was knocked out by a punch in the first round.[start] Él lo fue a la [UNK] por primera vez [end]\n",
      "In any case, I'll call you tomorrow.[start] cualquier nombre [UNK] en cualquier momento para nosotros [end]\n",
      "May I read the rest of the will now?[start] me ya podemos leer el descanso [end]\n",
      "He might change his mind.[start] Él podría [UNK] [end]\n",
      "This stable contains twelve stalls.[start] este a los [UNK] de a los [UNK] [end]\n",
      "Tom is my colleague.[start] tom es mi [UNK] [end]\n",
      "This is all you have to do now.[start] esto es todo lo que tienes que hacer ahora [end]\n",
      "What did you eat for dinner last night?[start] qué comiste para la navidad anoche [end]\n",
      "When will he be freed?[start] cuándo se [UNK] [end]\n",
      "Who said I had a gun?[start] quién dijo que yo tenía un arma [end]\n",
      "He believes whatever I say.[start] Él cree lo que digas [end]\n",
      "Do you like French wine?[start] le gusta el vino de vino [end]\n",
      "The subway entrance is on the corner.[start] el metro se [UNK] en la esquina [end]\n",
      "You can use my bicycle.[start] podéis usar mi bicicleta [end]\n",
      "I'm honored to work with Tom.[start] soy [UNK] para trabajar con tom [end]\n"
     ]
    }
   ],
   "source": [
    "spa_vocab = spa_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(30):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    translated = decode_sequence(input_sentence)\n",
    "    print(input_sentence + translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}